{"cells": [{"metadata": {}, "cell_type": "code", "source": "def vietnam_incubation():\n    import math\n    import re\n    import os\n    import ast \n    import json\n    import pandas as pd \n    import numpy as np\n    import time\n    from ibm_watson_machine_learning.foundation_models import Model\n    from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n    from ibm_watson import DiscoveryV2\n    from ibm_watson.discovery_v2 import DiscoveryV2, QueryLargePassages\n    from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n    \n    #========== watson discovery credentials========+\n    \n    DISCOVERY_USERNAME = \"apikey\"\n    DISCOVERY_APIKEY = \"xcKsSFr_8QRuo6Ahm7a5IMJIXgB4ypiuk099ta4wCKph\"\n    DISCOVERY_PROJECT = \"5e9472d7-d723-4b25-afc4-a1a92b83ce88\"\n    DISCOVERY_COLLECTION =\"91661ba7-cebc-ac5c-0000-018fec422801\"\n    DISCOVERY_URL = \"https://api.us-south.discovery.watson.cloud.ibm.com/instances/a1633f77-4808-4aa0-8354-2fa91a6d9957\"\n    DISCOVERY_VERSION = '2019-11-29'\n    authenticator = IAMAuthenticator(DISCOVERY_APIKEY)\n    DISCOVERY= DiscoveryV2(version=DISCOVERY_VERSION, authenticator=authenticator)\n    DISCOVERY.set_service_url(DISCOVERY_URL)\n    \n    #=========== watson discovery ====================+\n    \n    def query_data_only(question, max_per_document, collection_id):\n        collection_list = [collection_id]\n        user_question = question\n        passages = QueryLargePassages(enabled = True, per_document=True, find_answers=True, characters = 900, max_per_document=max_per_document, max_answers_per_passage=20)\n\n        query_result = DISCOVERY.query(\n            project_id=DISCOVERY_PROJECT,\n            # filter= f\"document_id::{document_id}\",\n            collection_ids=collection_list,\n            natural_language_query=user_question,\n            passages=passages).get_result()\n\n        combined_result = query_result\n        return combined_result\n\n    def regex_changer(query_result):\n        words_to_remove = ['<em>', '</em>']\n        pattern = '|'.join(re.escape(word) for word in words_to_remove)\n        output_string = re.sub(pattern, '', query_result)\n        return output_string\n\n\n    def query_data_json(combined_result):\n        json_file_list = []  # Create a list to store dictionaries\n\n        for i in range(len(combined_result['results'])):\n            doc_id = combined_result['results'][i]['document_id']\n            text = ''.join(combined_result['results'][i]['text'])\n            passage_text = []\n            confidence_score = 0  # Initialize confidence_score\n\n\n            for j in range(len(combined_result['results'][i]['document_passages'])):\n                max_j = len(combined_result['results'][i]['document_passages'])\n                start_offset = math.floor(combined_result['results'][i]['document_passages'][j]['start_offset'] / 1000) * 1000\n                end_offset = math.ceil(combined_result['results'][i]['document_passages'][j]['end_offset'] / 1000) * 1008\n                #bisa 1068\n                result = text[start_offset:end_offset]\n                combined_result_regex = regex_changer(result)\n                passage_text.append(combined_result_regex)\n\n                for k in range(len(combined_result['results'][i]['document_passages'][j]['answers'])):\n                    max_k = len(combined_result['results'][i]['document_passages'][j]['answers'])\n                    if max_j < 2 and k < len(combined_result['results'][i]['document_passages'][j]['answers']):\n                        # Correct indentation for confidence_score calculation\n                        confidence_score += combined_result['results'][i]['document_passages'][j]['answers'][k]['confidence']\n                        final_confidence_score = confidence_score / max_k\n\n                    elif max_j >=2 and k < len(combined_result['results'][i]['document_passages'][j]['answers']):\n                        if k < len(combined_result['results'][i]['document_passages'][j]['answers']):\n                            # print(f\"cf1:{combined_result['results'][i]['document_passages'][j]['answers'][k]['confidence']}\")\n                            confidence_score += combined_result['results'][i]['document_passages'][j]['answers'][k]['confidence']\n                        final_confidence_score = confidence_score/(max_j*max_k)\n                    else:\n                        print(\"should not used\")\n\n            if max_j < 2:\n                final_confidence_score = confidence_score / len(combined_result['results'][i]['document_passages'][j]['answers'])\n            elif max_j>=2:\n                final_confidence_score = final_confidence_score\n            else:\n                pass\n\n            json_file = {'doc_id': doc_id,\n                        'passage_text': passage_text,\n                        'confidence_score':final_confidence_score}\n\n\n            json_file_list.append(json_file) \n\n        return json_file_list\n    \n    #=========== watsonx.ai===========+\n    class WatsonXModel:\n        api_key = 'ZGBpAS7eDCkYb2H1IYdFB1S6xm-VCBWkXW_fiAfWQZLC'\n        project_id = \"5d038e3b-fbdd-4844-9985-7020d7fa88c7\"\n        ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\n\n        if api_key is None or ibm_cloud_url is None or project_id is None:\n            raise Exception(\"Ensure you copied the .env file that you created earlier into the same directory as this notebook\")\n        else:\n            creds = {\n                \"url\": ibm_cloud_url,\n                \"apikey\": api_key\n            }\n        def __init__(\n            self,\n            creds=creds,\n            project_id=project_id,\n            model_name=\"meta-llama/llama-3-70b-instruct\",\n            model_params = {\n                GenParams.DECODING_METHOD: \"greedy\",\n                GenParams.MIN_NEW_TOKENS: 1,\n                GenParams.MAX_NEW_TOKENS: 2000,\n                GenParams.TEMPERATURE: 0.75,\n                GenParams.REPETITION_PENALTY: 1,\n                GenParams.TOP_K: 1,\n                GenParams.RANDOM_SEED: 362,\n                GenParams.STOP_SEQUENCES: [\"[/INST]\", \" # \", \"<|endoftext|>\", \"========================\"],\n            }\n        ):\n            model = Model(\n                model_id=model_name,\n                params=model_params,\n                credentials=creds,\n                project_id=project_id\n            )\n\n            self.model = model\n        def qa_result(self, question, paragraph_text, conversation_history):\n            model = self.model\n            input_prompts = f\"\"\"\n            <|begin of text|>\n             <|start_header_id|>system<|end_header_id|>\n             You are an AI gene model for the Q&A task, you will be given a 'question' and answer the question only based on the 'text_paragraph' given.\n             Apart from that, there will be a 'conversation_history' which functions to provide context for previous conversations.\n             If you feel the new question is relevant to the previous question, link the answer. If not, just answer the question without relating it to 'conversation history'.\n\n             Paragraph_Text:\n             {paragraph_text}\n\n             conversation_history:\n             {conversation_history}\n\n             Answer the questions given by simply providing the answer, make sure the answer is detailed and comprehensive and extract the information you are trying to find from the conversation.\n             Write the answer in the following JSON format:\n             {{\"result\":{{\"question\":Asked question, \"answer\":\"answer to question\"}}}}\n             Make sure to only provide JSON answers.\n             <|eot_id|>\n\n             <|start_header_id|>example<|end_header_id|>\n             Question:\n             What is New Renewable Energy?\n             Answer:\n             {{\"result\":{{\"question\":\"What is new renewable energy?\",\"answer\":\"New and Renewable Energy (EBT) is a topic that is increasingly attracting attention throughout the world for the environmental and energy sectors. In the middle concerns about climate change and limited fossil resources, the potential for renewable energy in Indonesia has great opportunities\"}}}}\n\n             Question:\n             {question}\n             <|eot_id|>\n             Answer:\n            \"\"\"\n            generated_response = model.generate_text(prompt=input_prompts)\n            generated_response = generated_response.strip()\n            return generated_response\n\n        def relevancy_check_and_keyword(self, conversation_history, new_question):\n            model = self.model\n            input_prompts =f\"\"\"\n           <|begin of text|>\n             <|start_header_id|>system<|end_header_id|>\n             You are an AI gene model whose job is to see whether \"conversation_history\" is related to \"new_question\" and make changes to the question sentence asked\n             Here are some things that might happen:\n             1. If \"new_question\" uses the word reference, then \"new_question\" is relevant to \"convertsation_history\" so is_relevant is yes.\n             2. If the subject of \"new_question\" is the same as \"conversation_history\", then it is still relevant so is_relevant is yes.\n             3. If \"new_question\" is like following up on \"conversation_history\" fill in is_relevant with yes\n             4. If \"new_question\" does not contain a subject, then \"new_question\" is considered relevant to \"conversation_history\" so is_relevant is no.\n             5. If \"new_question\" asks something different from \"conversation_history\", then it is considered irrelevant so is_relevant is no.\n             6. If \"conversation_history\" is null and \"new_question\" has a value, then it is considered irrelevant is_relevant is no.\n             Give a response between yes or no.\n\n             If is_relevant is \"yes\" then change \"new_question\" by extracting information from \"convertsation_history question section\", \"conversation_history answer section\" and \"new_question\"\n             into a coherent question sentence and put it in the \"updated_question\" variable\n\n             If is_relevant is \"no\" the contents of \"updated_question\" remain the same as \"new_question\"\n             <|eot_id|>\n\n             <|start_header_id|>example<|end_header_id|>\n             Example 1:\n             new_question : What is the company's financial status?\n             conversation_history: f\"{{\\\"result\\\":{{\"question\":\"Explain everything you know about Pertamina\",\\\"answer\\\":\"Pertamina is a state-owned company operating in the energy sector and the largest dividend contributor in Indonesia. Having an integrated upstream to downstream business, starting from Exploration and Production, Processing, Distribution and Marketing, which is based on AKHLAK values \u200b\u200bas core values.\"}}}}\"\n             response: f\"{{{{\\\"result\\\":{{\\\"is_relevant\\\":\\\"yes\\\",\\\"updated_question\\\":\\\"What is Pertamina's financial status?\\\"}}}}}}\"\n\n             Example 2:\n             new_question: Can you explain in more detail?\n             conversation_history: f\"{{\\\"result\\\":{{\"question\":\"How much will Pertamina profit in 2022?\",\\\"answer\\\":\"Pertamina's profit achievement also made history, reaching IDR 60 trillion. He said, this also cannot be separated from the company's efforts to continue to make various efficiencies.\"}}}}\"\n             response: f\"{{{{\\\"result\\\":{{\\\"is_relevant\\\":\\\"yes\\\",\\\"updated_question\\\":\\\"Can you explain further regarding Pertamina's profit in 2022\\\"}} }}}}\"\n\n             Example 3:\n             new_question: What do you know about 3 Kg LPG gas?\n             convertsation_history: f\"{{\\\"result\\\":{{\"question\":\"What year was Pertamina founded?\",\\\"answer\\\":\"Pertamina was founded on December 10, 1957\"}}}}\"\n             response: f\"{{{{\\\"result\\\":{{\\\"is_relevant\\\":\\\"no\\\",\\\"updated_question\\\":\\\"What do you know about 3 Kg LPG gas?\\\"}} }}}}\"\n\n             Example 4:\n             new_question: What do you know about 3 Kg LPG gas?\n             conversation_history: null\n             response: f\"{{{{\\\"result\\\":{{\\\"is_relevant\\\":\\\"no\\\",\\\"updated_question\\\":\\\"What do you know about 3 Kg LPG gas?\\\"}} }}}}\"\n\n             Example 5:\n             new_question: How many subholdings does Pertamina have?\n             convertsation_history: f\"{{\\\"result\\\":{{\"question\":\"What is called OVOO?\",\\\"answer\\\":\"One Village One Outlet (OVOO) to expand the distribution of Liquified Petroleum Gas (LPG) three kilograms (kg) in North Sulawesi (Sulut) Province. \"Currently in North Sulawesi there are 4,052 3 kg LPG bases spread across 127 sub-districts or 1,408 sub-districts\"}}}}\"\n             response: f\"{{{{\\\"result\\\":{{\\\"is_relevant\\\":\\\"no\\\",\\\"updated_question\\\":\\\"How many subholdings is Pertamina?\\\"}}}}}}\"\n\n             Example 6:\n             new_question: What is the positive impact of this program\n             convertsation_history: f\"{{\\\"result\\\":{{\"question\":\"What is the One Price Fuel Program?\",\\\"answer\\\":\"Program to equalize fuel prices throughout Indonesia\"}}}}\"\n             response: f\"{{{{\\\"result\\\":{{\\\"is_relevant\\\":\\\"yes\\\",\\\"updated_question\\\":\\\"What is the positive impact of the One Price BBM program?\\\"}}}}}}\"\n             <|eot_id|>\n\n             <|start_header_id|>assistant<|end_header_id|>\n             Only provide responses in valid JSON format!\n\n             new_question: {new_question}\n             previous_question: {conversation_history}\n             response:\n             <|eot_id|>\n            \"\"\"\n\n            generated_response = model.generate_text(prompt=input_prompts)\n            generated_response = generated_response.strip()\n            return generated_response\n\n        \n    \n    def question_answer_flow(question, conversation_history):\n        wx = WatsonXModel()\n        try: \n            previous_question = conversation_history['result']['question']\n        except:\n            previous_question = \"\"\n        relevan_result = wx.relevancy_check_and_keyword(previous_question, question)\n        relevan_result = ast.literal_eval(relevan_result)\n        #display(relevan_result)\n        status_relevancy = relevan_result['result']['is_relevant']\n        if status_relevancy == \"no\":\n            conversation_history = \"\"\n            wd_result  = query_data_only(question=question, max_per_document=4, collection_id=DISCOVERY_COLLECTION)\n            wd_result = query_data_json(wd_result)\n            qa_result = wx.qa_result(question=question, paragraph_text =wd_result[0]['passage_text'], conversation_history=conversation_history)\n            qa_result = ast.literal_eval(qa_result)\n        else:\n            new_extracted_question = relevan_result['result']['updated_question']\n            wd_result  = query_data_only(question=new_extracted_question, max_per_document=4, collection_id=DISCOVERY_COLLECTION)\n            wd_result = query_data_json(wd_result)\n            qa_result = wx.qa_result(question=new_extracted_question, paragraph_text =wd_result[0]['passage_text'], conversation_history=conversation_history)\n            qa_result = ast.literal_eval(qa_result)\n        conversation_history = qa_result\n        return qa_result, conversation_history, status_relevancy, previous_question, wd_result\n    \n    #================= Main Functions ================+\n    def vietnam_incubation(payload):\n        start_time = time.time()\n        json_qna = {}\n        for data in payload['input_data']:\n            values = data['values'][0][0]\n            question = values['question']\n            conversation_history = values['conversation_history']\n            qa_result, conversation_history_final, status_relevancy, prev_question, wd_result = question_answer_flow(question, conversation_history)\n            json_qna['qa_result'] = qa_result\n            json_qna['conversation_history'] = conversation_history_final\n            json_qna['status_relevancy'] = status_relevancy\n            json_qna['previous_question'] = prev_question\n            json_payload_result = []\n            json_payload_result.append(json_qna)\n            json_payload_result\n        end_time = time.time()\n        delta_time_sec = end_time - start_time\n        return {'predictions': [{'fields': ['result'], 'values': [{'process_result':json_payload_result, 'eta':delta_time_sec }]}]}\n    return vietnam_incubation", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "question = \"Where is the experiment is carried out?\"\nconversation_history = {'result': {\"question\" : \"How much water absorb?\",\n                                    'answer': 'It is absorb 97% water to transport system'}}\n# conversation_history = \"\"\npayload = {\n    \"input_data\": [{\n        \"fields\": [\"query\"],\n        \"values\": [[{\"question\":question, \"conversation_history\":conversation_history}]]\n}]}\nvietnam_incubation()(payload)", "execution_count": 16, "outputs": [{"output_type": "display_data", "data": {"text/plain": "{'result': {'is_relevant': 'no',\n  'updated_question': 'Where is the experiment is carried out?'}}"}, "metadata": {}}, {"output_type": "execute_result", "execution_count": 16, "data": {"text/plain": "{'predictions': [{'fields': ['result'],\n   'values': [{'process_result': [{'qa_result': {'result': {'question': 'Where is the experiment carried out?',\n         'answer': 'The experiment was carried out in a greenhouse at the Federal University of S\u00e3o Jo\u00e3o del-Rei (UFSJ), in the municipality of Sete Lagoas, Minas Gerais State, Brazil (19\u00b0 28\u2019 32\u201d S, 44\u00b0 11\u2019 44\u201d W).'}},\n       'conversation_history': {'result': {'question': 'Where is the experiment carried out?',\n         'answer': 'The experiment was carried out in a greenhouse at the Federal University of S\u00e3o Jo\u00e3o del-Rei (UFSJ), in the municipality of Sete Lagoas, Minas Gerais State, Brazil (19\u00b0 28\u2019 32\u201d S, 44\u00b0 11\u2019 44\u201d W).'}},\n       'status_relevancy': 'no',\n       'previous_question': 'How much water absorb?'}],\n     'eta': 8.622572660446167}]}]}"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "api_key = 'ZGBpAS7eDCkYb2H1IYdFB1S6xm-VCBWkXW_fiAfWQZLC'\nproject_id = \"1eb14bcf-6e24-47ec-a8a4-7748def62568\"\nibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\nspace_id = \"7ee16482-8596-4c98-9a45-0f96e3eeec15\"\nlocation = \"us-south\"\nwml_credentials = {\n    \"apikey\": api_key,\n    \"url\": 'https://' + location + '.ml.cloud.ibm.com'\n}\nfrom ibm_watson_machine_learning import APIClient\nimport requests\nimport json\n\nclient = APIClient(wml_credentials)", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#if you want to redeploy just re run this model\nclient.set.default_space(space_id)\nsofware_spec_uid = client.software_specifications.get_id_by_name(\"incu-vietnam\")\nmeta_data = {\n    client.repository.FunctionMetaNames.NAME:'Incubation-Vietnam-V3',\n    client.repository.FunctionMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid\n}\nfunction_details = client.repository.store_function(meta_props=meta_data, function=vietnam_incubation)", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "function_uid = client.repository.get_function_uid(function_details)\n# Deploy the stored function\n\nmetadata = {\n    client.deployments.ConfigurationMetaNames.NAME: \"Vietnam-Incubation\",\n    client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\nfunction_deployment_details = client.deployments.create(function_uid, meta_props=metadata)", "execution_count": 20, "outputs": [{"output_type": "stream", "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: 'f7e4e78d-e783-4824-9b56-7b9dcaef79dc' started\n\n#######################################################################################\n\n\ninitializing\nNote: online_url and serving_urls are deprecated and will be removed in a future release. Use inference instead.\n....................................\nready\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='226dc1b6-54a4-4f71-b35b-f436158d1bc2'\n------------------------------------------------------------------------------------------------\n\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# Get the endpoint URL of the function deployment just created\nfunction_deployment_id = client.deployments.get_uid(function_deployment_details)\nfunction_deployment_endpoint_url = client.deployments.get_scoring_href(function_deployment_details)\nprint(\"Function deployment id: {}\".format(function_deployment_id))\nprint(\"Endpoint URL: {}\".format(function_deployment_endpoint_url))", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "Function deployment id: 226dc1b6-54a4-4f71-b35b-f436158d1bc2\nEndpoint URL: https://us-south.ml.cloud.ibm.com/ml/v4/deployments/226dc1b6-54a4-4f71-b35b-f436158d1bc2/predictions\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}